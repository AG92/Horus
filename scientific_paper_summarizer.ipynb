{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc5da5cd",
   "metadata": {},
   "source": [
    "# Scientific Paper Summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381f3925",
   "metadata": {},
   "source": [
    "The script aims to find and summarize papers based on a specific\n",
    "\n",
    "Process:\n",
    "1. Download the paper\n",
    "2. Convert from pdf to text\n",
    "3. Feed the text to the GPT-3 model using the openai api\n",
    "4. Show the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec4a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "\n",
    "url = 'https://www.sciencedaily.com/releases/2021/08/210811162816.htm'\n",
    "article = Article(url)\n",
    "article.download()\n",
    "article.parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9564df",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a9a479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "\n",
    "def summarize(text, per):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc= nlp(text)\n",
    "    tokens=[token.text for token in doc]\n",
    "    word_frequencies={}\n",
    "    for word in doc:\n",
    "        if word.text.lower() not in list(STOP_WORDS):\n",
    "            if word.text.lower() not in punctuation:\n",
    "                if word.text not in word_frequencies.keys():\n",
    "                    word_frequencies[word.text] = 1\n",
    "                else:\n",
    "                    word_frequencies[word.text] += 1\n",
    "    max_frequency=max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word]=word_frequencies[word]/max_frequency\n",
    "    sentence_tokens= [sent for sent in doc.sents]\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_tokens:\n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "                if sent not in sentence_scores.keys():                            \n",
    "                    sentence_scores[sent]=word_frequencies[word.text.lower()]\n",
    "                else:\n",
    "                    sentence_scores[sent]+=word_frequencies[word.text.lower()]\n",
    "    select_length=int(len(sentence_tokens)*per)\n",
    "    summary=nlargest(select_length, sentence_scores,key=sentence_scores.get)\n",
    "    final_summary=[word.text for word in summary]\n",
    "    summary=''.join(final_summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86f58e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    " \n",
    "def read_article(file_name):\n",
    "    file = open(file_name, \"r\")\n",
    "    filedata = file.readlines()\n",
    "    article = filedata[0].split(\". \")\n",
    "    sentences = []\n",
    "\n",
    "    for sentence in article:\n",
    "        print(sentence)\n",
    "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
    "    sentences.pop() \n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    " \n",
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def generate_summary(file_name, top_n=5):\n",
    "    nltk.download(\"stopwords\")\n",
    "    stop_words = stopwords.words('english')\n",
    "    summarize_text = []\n",
    "\n",
    "    # Step 1 - Read text anc split it\n",
    "    sentences =  read_article(file_name)\n",
    "\n",
    "    # Step 2 - Generate Similary Martix across sentences\n",
    "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
    "\n",
    "    # Step 3 - Rank sentences in similarity martix\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "\n",
    "    # Step 4 - Sort the rank and pick top sentences\n",
    "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
    "    print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
    "\n",
    "    for i in range(top_n):\n",
    "      summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
    "\n",
    "    # Step 5 - Offcourse, output the summarize text\n",
    "    print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n",
    "\n",
    "# let's begin\n",
    "generate_summary( \"msft.txt\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b173d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import wget\n",
    "import pathlib\n",
    "import pdfplumber\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8653372d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-0.19.0.tar.gz (42 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting pandas-stubs>=1.1.0.11\n",
      "  Downloading pandas_stubs-1.2.0.60-py3-none-any.whl (162 kB)\n",
      "Requirement already satisfied: pandas>=1.2.3 in c:\\users\\aron gosch\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openai) (1.4.1)\n",
      "Collecting openpyxl>=3.0.7\n",
      "  Downloading openpyxl-3.0.10-py2.py3-none-any.whl (242 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aron gosch\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openai) (4.62.3)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\aron gosch\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openai) (2.27.1)\n",
      "Collecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\aron gosch\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas>=1.2.3->openai) (1.22.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\aron gosch\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas>=1.2.3->openai) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aron gosch\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas>=1.2.3->openai) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aron gosch\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.2.3->openai) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\aron gosch\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.20->openai) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\aron gosch\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.20->openai) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aron gosch\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.20->openai) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aron gosch\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.20->openai) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\aron gosch\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm->openai) (0.4.4)\n",
      "Building wheels for collected packages: openai\n",
      "  Building wheel for openai (PEP 517): started\n",
      "  Building wheel for openai (PEP 517): finished with status 'done'\n",
      "  Created wheel for openai: filename=openai-0.19.0-py3-none-any.whl size=53511 sha256=62b655bb7b93ea5880994c1c0d42fae03c5ea607ee023a3b66fc5aeb90728114\n",
      "  Stored in directory: c:\\users\\aron gosch\\appdata\\local\\pip\\cache\\wheels\\02\\b0\\11\\038d87bdba9a4a9dfd20b1e24a643bd3bc9de8b24621a8c063\n",
      "Successfully built openai\n",
      "Installing collected packages: et-xmlfile, pandas-stubs, openpyxl, openai\n",
      "Successfully installed et-xmlfile-1.1.0 openai-0.19.0 openpyxl-3.0.10 pandas-stubs-1.2.0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\aron gosch\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "Using legacy 'setup.py install' for wget, since package 'wheel' is not installed.\n",
      "Installing collected packages: wget\n",
      "    Running setup.py install for wget: started\n",
      "    Running setup.py install for wget: finished with status 'done'\n",
      "Successfully installed wget-3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\aron gosch\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.7.1-py3-none-any.whl (39 kB)\n",
      "Collecting Wand>=0.6.7\n",
      "  Downloading Wand-0.6.7-py2.py3-none-any.whl (139 kB)\n",
      "Collecting pdfminer.six==20220524\n",
      "  Downloading pdfminer.six-20220524-py3-none-any.whl (5.6 MB)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\aron gosch\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting Pillow>=9.1\n",
      "  Downloading Pillow-9.1.1-cp39-cp39-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\aron gosch\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pdfminer.six==20220524->pdfplumber) (2.0.12)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\aron gosch\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pdfminer.six==20220524->pdfplumber) (36.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\aron gosch\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20220524->pdfplumber) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\aron gosch\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20220524->pdfplumber) (2.21)\n",
      "Installing collected packages: Wand, Pillow, pdfminer.six, pdfplumber\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: Pillow 9.0.1\n",
      "    Uninstalling Pillow-9.0.1:\n",
      "      Successfully uninstalled Pillow-9.0.1\n",
      "Successfully installed Pillow-9.1.1 Wand-0.6.7 pdfminer.six-20220524 pdfplumber-0.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n",
    "!pip install wget\n",
    "!pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "834eab84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import wget\n",
    "import pathlib\n",
    "import pdfplumber\n",
    "import numpy as np\n",
    "\n",
    "def getPaper(paper_url, filename=\"random_paper.pdf\"):\n",
    "    \"\"\"\n",
    "    Downloads a paper from it's arxiv page and returns\n",
    "    the local path to that file.\n",
    "    \"\"\"\n",
    "    downloadedPaper = wget.download(paper_url, filename)    \n",
    "    downloadedPaperFilePath = pathlib.Path(downloadedPaper)\n",
    "\n",
    "    return downloadedPaperFilePath\n",
    "\n",
    "def displayPaperContent(paperContent, page_start=0, page_end=5):\n",
    "    for page in paperContent[page_start:page_end]:\n",
    "        print(page.extract_text())\n",
    "\n",
    "def showPaperSummary(paperContent):\n",
    "    with open('openAI_key.txt') as f:\n",
    "        key = f.readlines()\n",
    "    \n",
    "    tldr_tag = \"\\n tl;dr:\"\n",
    "    openai.organization = ''\n",
    "    openai.api_key = key[0]\n",
    "    engine_list = openai.Engine.list() \n",
    "    \n",
    "    for page in paperContent:    \n",
    "        text = page.extract_text() + tldr_tag\n",
    "        response = openai.Completion.create(engine=\"davinci\",prompt=text,temperature=0.3,\n",
    "            max_tokens=140,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=[\"\\n\"]\n",
    "        )\n",
    "        print(response[\"choices\"][0][\"text\"])\n",
    "\n",
    "#paperContent = pdfplumber.open(paperFilePath).pages\n",
    "#showPaperSummary(paperContent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e5ca5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [......................................................] 1108543 / 1108543Understanding training and generalization in deep\n",
      "learning by Fourier analysis\n",
      "Zhi-QinJohnXu∗\n",
      "8\n",
      "NewYorkUniversityAbuDhabi\n",
      "1\n",
      "AbuDhabi129188,UnitedArabEmirates\n",
      "0\n",
      "zhiqinxu@nyu.edu\n",
      "2\n",
      " \n",
      "v\n",
      "o\n",
      "N Abstract\n",
      " \n",
      "9\n",
      "2 Background: It is still an open research area to theoretically understand why\n",
      "  DeepNeuralNetworks(DNNs)—equippedwithmanymoreparametersthantrain-\n",
      " \n",
      "] ing data and trained by (stochastic) gradient-based methods—often achieve re-\n",
      "G\n",
      "markably low generalization error. Contribution: We study DNN training by\n",
      "L Fourier analysis. Our theoretical frameworkexplains: i) DNN with (stochastic)\n",
      ". gradient-based methods often endows low-frequency components of the target\n",
      "s\n",
      "c function with a higher priority during the training; ii) Small initialization leads\n",
      "[ to good generalizationability of DNN while preservingthe DNN’s ability to ﬁt\n",
      " \n",
      "  any function. These results are further conﬁrmed by experiments of DNNs ﬁt-\n",
      "4\n",
      "tingthefollowingdatasets,thatis,naturalimages,one-dimensionalfunctionsand\n",
      "v\n",
      "MNISTdataset.\n",
      "5\n",
      "9\n",
      "2\n",
      "4 1 Introduction\n",
      "0\n",
      ".\n",
      "8 Background Deep learning has achieved great success as in many ﬁelds (LeCunetal. (2015)).\n",
      "0 Recent studies have focused on understandingwhy DNNs, trained by (stochastic) gradient-based\n",
      "8\n",
      "methods, can generalize well, that is, DNNs often ﬁt the test data well which are not used for\n",
      "1\n",
      "traininginpractice. Counter-intuitively,althoughDNNshavemanymoreparametersthantraining\n",
      ":\n",
      "v data,theycanrarelyoverﬁtthetrainingdatainpractice.\n",
      "i\n",
      "X Several studies have focused on the local property (sharpness/ﬂatness) of loss function at min-\n",
      "r ima (Hochreiter&Schmidhuber(1995)) to explorethe DNN’s generalizationability. Keskaretal.\n",
      "a (2016)empiricallydemonstratedthatwithsmallbatchineachtrainingstep,DNNsconsistentlycon-\n",
      "verge to ﬂat minima, and lead to a better generalization. However, Dinhetal. (2017) arguedthat\n",
      "most notionsof ﬂatness are problematic. To this end, Dinhetal. (2017) used deep networkswith\n",
      "rectiﬁer linear units (ReLUs) to theoretically show that any minimum can be arbitrarily sharp or\n",
      "ﬂatwithoutspecifyingparameterization. Withtheconstraintofsmallweightsin parameterization,\n",
      "Wuetal.(2017)provedthatfortwo-layerReLUnetworks,low-complexitysolutionslieintheareas\n",
      "withsmallHessian,thatis,ﬂatandlargebasinsofattractor(Wuetal.(2017)).Theythenconcluded\n",
      "thatarandominitializationtendstoproducestartingparameterslocatedinthebasinofﬂatminima\n",
      "withahighprobability,usinggradient-basedmethods.\n",
      "Several studies rely on the concept of stochastic approximation or uniform stability\n",
      "(Bousquet&Elisseeff (2002), Hardtetal. (2015)). To ensure the stability of a training algorithm,\n",
      "Hardtetal.(2015)assumedlossfunctionwithgoodproperties,suchasLipschitzorsmoothcondi-\n",
      "tions. However,thelossfunctionofaDNNisoftenverycomplicated(Zhangetal.(2016)).\n",
      "∗ThisworkisdonewhileXuisavisitingmemberatCourantInstituteofMathematicalSciences,NewYork\n",
      "University,NewYork,UnitedStates.\n",
      "Preprint.Workinprogress.\n",
      "Another approach to understanding the DNN’s generalization ability is to ﬁnd general principles\n",
      "duringtheDNN’straining. Empirically,Arpitetal.(2017)suggestedthatDNNsmaylearnsimple\n",
      "patternsﬁrstinrealdata,beforememorizing. Xuetal.(2018)foundasimilarphenomenonempiri-\n",
      "cally,whichisreferredtoFrequencyPrinciple(F-Principle),thatis,foralow-frequencydominant\n",
      "function, DNNs with common settings ﬁrst quickly capture the dominant low-frequency compo-\n",
      "nentswhilekeepingtheamplitudesofhigh-frequencycomponentssmall,andthenrelativelyslowly\n",
      "capturesthosehigh-frequencycomponents.F-PrinciplecanexplainhowthetrainingcanleadDNNs\n",
      "toagoodgeneralizationempiricallybyshowingthattheDNNpreferstoﬁtthetargetfunctionbya\n",
      "low-complexityfunctionXuetal.(2018). Rahamanetal.(2018)foundasimilarresultthat“Lower\n",
      "FrequenciesareLearnedFirst”. Tounderstandthisphenomenon,Rahamanetal. (2018) estimated\n",
      "aninequalitythatthe amplitudeofeachfrequencycomponentofthe DNNoutputiscontrolledby\n",
      "thespectralnormofDNNweightstheoretically. Rahamanetal.(2018)thenshowthatthespectral\n",
      "norm2 increases gradually during training with a small-size DNN empirically. Therefore, the in-\n",
      "equality implies that “longer training allows the network to represent more complex functionsby\n",
      "allowingittoalsoﬁthigherfrequencies”.However,foralarge-sizeDNN,thespectralnormalmost\n",
      "doesnotchangeduringthetraining(SeeanexampleinFig.5inAppendix.),thatis,theboundofthe\n",
      "amplitudeofeachfrequencycomponentoftheDNNoutputalmostdoesnotchangeduringthetrain-\n",
      "ing.Then,theinequalityinRahamanetal.(2018)cannotexplainwhytheF-Principlestillholdsfor\n",
      "alarge-sizeDNN.\n",
      "Contribution In this work, we develop a theoretical framework by Fourier analysis aiming to\n",
      "understandthetrainingprocessandthegeneralizationabilityofDNNswithsufﬁcientneuronsand\n",
      "hiddenlayers. We showthatforanyparameter,thegradientdescentmagnitudein eachfrequency\n",
      "componentof the loss functionis proportionalto the productof two factors: one is a decay term\n",
      "with respect to (w.r.t.) frequency; the other is the amplitude of the difference between the DNN\n",
      "output and the target function. This theoretical framework shows that DNNs trained by gradient-\n",
      "based methodsendowlow-frequencycomponentswith higher priorityduringthe training process.\n",
      "Since the power spectrum of the tanh function exponentially decays w.r.t. frequency, in which\n",
      "theexponentialdecayrateisproportionaltotheinverseofweight. Wethenshowthatsmall(large)\n",
      "initializationwouldresultinsmall(large)amplitudeofhigh-frequencycomponents,thusleadingthe\n",
      "DNNoutputtoalow(high)complexityfunctionwithgood(bad)generalizationability. Therefore,\n",
      "withsmallinitialization,sufﬁcientlargeDNNscanﬁtanyfunction(Cybenko(1989))whilekeeping\n",
      "goodgeneralization.\n",
      "We demonstratethattheanalysisinthisworkcanbequalitativelyextendedto generalDNNs. We\n",
      "exempliﬁedourtheoreticalresultsthroughDNNsﬁttingnaturalimages,1-dfunctionsandMNIST\n",
      "dataset(LeCun(1998)).\n",
      "The paperis established as follows. The commonsettings of DNNs in this work are presentedin\n",
      "Section2. ThetheoreticalframeworkisgiveninSection3. Wethenstudytheevolutionofthemean\n",
      "magnitudeof DNN parametersduringthe DNN trainingempiricallyin Section 4. The theoretical\n",
      "frameworkisvalidatedbyexperimentsinSection5. Theconclusionsanddiscussionsarefollowed\n",
      "inSection6.\n",
      "2 Methods\n",
      "The activation functionfor each neuronis tanh. We use DNNs of multiple hidden layers with no\n",
      "activation function for the output layer. The DNN is trained by Adam optimizer (Kingma&Ba\n",
      "(2014)).ParametersoftheAdamoptimizeraresettothedefaultvalues(Kingma&Ba(2014)).The\n",
      "loss function is the mean squared error of the difference between the DNN output and the target\n",
      "functioninthetrainingset.\n",
      "2InRahamanetal.(2018),“formatrix-valuedweights,theirspectralnormwascomputedbyevaluatingthe\n",
      "eigenvalueoftheeigenvectorobtainedwith10poweriterations. Forvector-valuedweights,wesimplyusethe\n",
      "L norm”.\n",
      "2\n",
      "2\n",
      "3 Theoretical framework\n",
      "In this section, we will develop a theoretical framework in the Fourier domain to understand the\n",
      "training process of DNN. For illustration, we ﬁrst use a DNN with one hidden layer with tanh\n",
      "functionσ(x)astheactivationfunction:\n",
      "ex−e−x\n",
      "σ(x)=tanh(x)= , x∈R.\n",
      "e−x+ex\n",
      "TheFouriertransformofσ(wx+b)withw,b∈Ris,\n",
      "π π i 1\n",
      "F[σ(wx+b)](k)= δ(k)+ exp(−ibk/w) , (1)\n",
      "r2 r2 |w| exp(πk/2w)−exp(−πk/2w)\n",
      "where\n",
      "∞\n",
      "F[σ(x)](k)= σ(x)exp(−ikx)dx.\n",
      "Z−∞\n",
      "ConsideraDNNwithonehiddenlayerwithN nodes,1-dinputxand1-doutput:\n",
      "N\n",
      "ϒ(x)= ∑a σ(w x+b ), a ,w ,b ∈R. (2)\n",
      "j j j j j j\n",
      "j=1\n",
      "Notethatwecallallw ,a andb asparameters,inwhichw anda areweights,andb isabias\n",
      "j j j j j j\n",
      "term. When|πk/w |islarge,withoutlossofgenerality,weassumeπk/w ≫0andw >0,\n",
      "j j j\n",
      "N π i\n",
      "F[ϒ](k)≈ ∑a exp(−(ib +π/2)k/w ) . (3)\n",
      "j=1 j(cid:20)r2 wj j j (cid:21)\n",
      "WedeﬁnethedifferencebetweenDNNoutputandthetargetfunction f(x)ateachkas\n",
      "D(k),F[ϒ](k)−F[f](k).\n",
      "WriteD(k)as\n",
      "D(k)=A(k)eiθ(k), (4)\n",
      "whereA(k)andθ(k)∈[−π,π],indicatetheamplitudeandphaseofD(k),respectively. Thelossat\n",
      "frequencyk is L(k)= 1|D(k)|2, where |·| denotesthe normof a complexnumber. The totalloss\n",
      "2\n",
      "functionisdeﬁnedas:\n",
      "L=∑L(k).\n",
      "k\n",
      "NotethataccordingtotheParseval’stheorem3,thislossfunctionintheFourierdomainisequalto\n",
      "thecommonlyusedlossofmeansquarederror,thatis,\n",
      "1\n",
      "L= ∑(ϒ(x)−f(x))2, (5)\n",
      "2\n",
      "x\n",
      "At frequencyk, the amplitudeof the gradientwith respectto each parametercan be obtained(see\n",
      "Appendixfordetails),\n",
      "∂L(k) C\n",
      "0\n",
      "=i(C −1) A(k)exp(−πk/2w ) (6)\n",
      "1 j\n",
      "∂a w\n",
      "j j\n",
      "∂L(k) aj\n",
      "=C C A(k)exp(−πk/2w ) (7)\n",
      "∂wj 0 22w3j j\n",
      "∂L(k) ajk\n",
      "=(C −1)C A(k)exp(−πk/2w ), (8)\n",
      "∂b 1 0 w2 j\n",
      "j j\n",
      "where\n",
      "π\n",
      "C = ei[θ(k)+bjk/wj] (9)\n",
      "0 r2\n",
      "3Without loss of generality, the constant factor that is related to the deﬁnition of corresponding Fourier\n",
      "Transformisignoredhere.\n",
      "3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C =exp(−2i(b k/w +θ(k))), (10)\n",
      "1 j j\n",
      "C =[C (i(πk−2w )−2b k)+(−i(πk−2w )−2b k)], (11)\n",
      "2 1 j j j j\n",
      "Thedescentamountatanydirection,say,withrespecttoparameterΘ ,is\n",
      "jl\n",
      "∂L ∂L(k)\n",
      "=∑ . (12)\n",
      "∂Θ ∂Θ\n",
      "jl l jl\n",
      "TheabsolutecontributionfromfrequencyktothistotalamountatΘ is\n",
      "jl\n",
      "∂L(k)\n",
      "=A(k)exp(−|πk/2w |)G (Θ ,k), (13)\n",
      "j jl j\n",
      "(cid:12)∂Θ (cid:12)\n",
      "(cid:12) jl (cid:12)\n",
      "(cid:12) (cid:12)\n",
      "whereΘ ,{w ,b ,a },Θ(cid:12) ∈Θ ,(cid:12)G (Θ ,k)isafunctionwithrespecttoΘ andk,whichcanbe\n",
      "j j j j jl j jl j j\n",
      "foundinoneofEqs. (6,7,8).\n",
      "When the component at frequency k does not converge yet, exp(−|πk/2w |) would dominate\n",
      "j\n",
      "G (Θ ,k)forasmallw .Therefore,thebehaviorofEq.(13)isdominatedbyA(k)exp(−|πk/2w |).\n",
      "jl j j j\n",
      "Thisdominanttermalsoindicatesthatweightsaremuchmoreimportantthanbiasterms,whichwill\n",
      "beveriﬁedbyMNISTdatasetlater.\n",
      "To examine the convergencebehavior of different frequency components during the training, we\n",
      "computetherelativedifferenceoftheDNNoutputand f(x)inthefrequencydomainateachrecord-\n",
      "ingstep,thatis,\n",
      "|F[f](k)−F[ϒ](k)|\n",
      "∆ (k)= . (14)\n",
      "F\n",
      "|F[f](k)|\n",
      "Throughtheaboveanalysisframework,wehavethefollowingtheorems(Theproofscanbefound\n",
      "atAppendix.)\n",
      "Theorem 1. Consider a DNN with one hidden layer using tanh function σ(x) as the activation\n",
      "function. For any frequencies k and k such that k > k >0 and there exist c ,c , such that\n",
      "1 2 2 1 1 2\n",
      "A(k )>c >0,A(k )<c <∞,wehave\n",
      "1 1 2 2\n",
      "µ w : ∂L(k1) > ∂L(k2) forall j,l ∩B\n",
      "lim (cid:16)n j (cid:12) ∂Θjl (cid:12) (cid:12) ∂Θjl (cid:12) o δ(cid:17) =1, (15)\n",
      "δ→0 (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) µ(Bδ(cid:12)(cid:12))\n",
      "whereB isaballwithradiusδcenteredattheoriginandµ(·)istheLebesguemeasureofaset.\n",
      "δ\n",
      "Thm 1 shows that for any two non-converged frequencies, almost all sufﬁciently small weights\n",
      "satisfythatalower-frequencycomponenthasahigherpriorityduringthegradient-basedtraining.\n",
      "Theorem 2. Consider a DNN with one hidden layer with tanh function σ(x) as the activation\n",
      "function.Foranyfrequenciesk andk suchthatk >k >0,weconsidernon-degeneratesituation,\n",
      "1 2 2 1\n",
      "that is, |F[f](k )|>0, and there exist positive constantsC , ξ, ξ, andξ, such that A(k )<C ,\n",
      "1 a 1 2 2 a\n",
      "|1−C (k )|>ξ,andξ >|C (k )|>ξ. ∀ε>0. Then,foranyε>0,thereexistsM>0,forany\n",
      "1 1 2 2 1 1\n",
      "w ∈[−M,M]\\{0}suchthatthereexistsΘ ∈{w ,b ,a }satisfying\n",
      "j jl j j j\n",
      "∂L(k ) ∂L(k )\n",
      "1 2\n",
      "= , (16)\n",
      "(cid:12) ∂Θ (cid:12) (cid:12) ∂Θ (cid:12)\n",
      "(cid:12) jl (cid:12) (cid:12) jl (cid:12)\n",
      "(cid:12) (cid:12) (cid:12) (cid:12)\n",
      "wehave∆ (k )<ε. (cid:12) (cid:12) (cid:12) (cid:12)\n",
      "F 1\n",
      "The case that the gradient amplitude of the low-frequency component equals to that of the high-\n",
      "frequency component indicates that low frequency does not dominate. Thm 2 implies that when\n",
      "the deviation of the high-frequency component (A(k )) is bounded, we can always ﬁnd small\n",
      "2\n",
      "weights such that the relative error of the low-frequency component stays very small when the\n",
      "high-frequency component has the similar priority as the low-frequency component. In another\n",
      "word,whentheDNNstartstoﬁtthehigh-frequencycomponent,thelow-frequencyonestaysatthe\n",
      "convergedstate.\n",
      "Theorem 3. Consider a DNN with one hidden layer with tanh function σ(x) as the activation\n",
      "function.Foranyfrequenciesk andk suchthatk >k >0,weconsidernon-degeneratesituation,\n",
      "1 2 2 1\n",
      "thatis, |F[f](k )|>0, andthereexist positiveconstantsξ,ξ, andξ, suchthat|1−C (k )|>ξ\n",
      "1 1 2 1 1\n",
      "4\n",
      "ngth0.08 weight ngth ngth0.8\n",
      "s stre0.06 bias s stre0.2 s stre0.6\n",
      "b b b\n",
      "n of a0.04 n of a0.1 n of a0.4\n",
      "ea0.02 ea ea0.2\n",
      "m m m\n",
      "0 100 200 300 0 200 400 600 0 250 500 750 1000\n",
      "Epoch Epoch Epoch\n",
      "(a) std:0.06 (b) std:0.2 (c) std:0.6\n",
      "Figure 1: Magnitudeof DNN parametersduringﬁtting MNIST dataset. DNN parametersare ini-\n",
      "tialized by Gaussian distribution with mean 0 and standard deviation 0.06, 0.2, 0.6 for (a, b, c),\n",
      "respectively. Solidslinesshowthemeanmagnitudeoftheabsoluteweights(red)andtheabsolute\n",
      "bias(green)ateachtrainingepoch. Thedashedlinesarethemean±stdforthecorrespondingcolor.\n",
      "Notethatthegreenandtheredlinesalmostoverlap. WeuseatanhDNNwithwidth: 800-400-200-\n",
      "100.Thelearningrateis10−5withbatchsize400.\n",
      "andξ >|C (k )|>ξ,foranyB>0,thereexistsM>0,foranyA(k )>M,suchthatthereexists\n",
      "2 2 1 1 2\n",
      "Θ ∈{w ,b ,a }satisfying\n",
      "jl j j j\n",
      "∂L(k ) ∂L(k )\n",
      "1 2\n",
      "= 6=0, (17)\n",
      "(cid:12) ∂Θ (cid:12) (cid:12) ∂Θ (cid:12)\n",
      "(cid:12) jl (cid:12) (cid:12) jl (cid:12)\n",
      "(cid:12) (cid:12) (cid:12) (cid:12)\n",
      "wehave∆ (k )>B. (cid:12) (cid:12) (cid:12) (cid:12)\n",
      "F 1\n",
      "Thm 3 implies that for a large A(k ), when the low-frequency componentdoes not converge yet,\n",
      "2\n",
      "it alreadycan be signiﬁcantlyaffectedby the high-frequencycomponent. A large A(k ) can exist\n",
      "2\n",
      "when the targetfunctionis high-frequencydominate, thatis, a higher-frequencycomponenthas a\n",
      "largeramplitude(SeeanexampleinAppendixE).\n",
      "Next,wedemonstratethattheanalysisofEq. (13)canbequalitativelyextendedtogeneralDNNs.\n",
      "A(k) in Eq. (13) comes from the square operation in the loss function, thus, is irrelevant with\n",
      "DNN structure. exp(−|πk/2w |) in Eq. (13) comesfrom the exponentialdecay of the activation\n",
      "j\n",
      "function in the Fourier domain. The analysis of exp(−|πk/2w |) is insensitive to the following\n",
      "j\n",
      "factors. i) Activationfunction. The powerspectrumof mostactivationfunctionsdecreasesas the\n",
      "frequencyincreases;ii)Neuronnumber. ThesummationinEq. (2)doesnotaffecttheexponential\n",
      "decay;iii)Multiplehiddenlayers.Iftherearemultiplehiddenlayers,thecompositionofcontinuous\n",
      "activationfunctionsis stilla continuousfunction. The powerspectrumofthe continuousfunction\n",
      "stilldecaysasthefrequencyincreases;iv)High-dimensionalinput. Weneedtoconsiderthevector\n",
      "formofk inEq. (1); v) High-dimensionaloutput. Thetotallossfunctionisthesummationof the\n",
      "lossofeachoutputnode. Therefore,theanalysisofA(k)exp(−|πk/2w |)ofasinglehiddenlayer\n",
      "j\n",
      "qualitativelyappliestodifferentactivationfunctions,neuronnumbers,multiplehiddenlayers,and\n",
      "high-dimensionalfunctions.\n",
      "4 The magnitude ofDNN parameters during training\n",
      "Since the magnitude of DNN parametersis importantto the analysis of the gradients, such as w\n",
      "j\n",
      "inEq. (13),westudytheevolutionofthemagnitudeofDNNparametersduringtraining. Through\n",
      "trainingDNNsbyMNISTdataset,empirically,weshowthatforanetworkwithsufﬁcientneurons\n",
      "and layers, the mean magnitude of the absolute values of DNN parameters only changes slightly\n",
      "duringthetraining.Forexample,wetrainaDNNbyMNISTdatasetwithdifferentinitialization.In\n",
      "Fig.1,DNNparametersareinitializedbyGaussiandistributionwithmean0andstandarddeviation\n",
      "0.06, 0.2, 0.6 for (a, b, c), respectively. We compute the mean magnitude of absolute weights\n",
      "andbiasterms. AsshowninFig.1, themeanmagnitudeoftheabsolutevalueofDNN parameters\n",
      "onlychangesslightlyduringthetraining.Thus,empirically,theinitializationalmostdeterminesthe\n",
      "magnitudeof DNN parameters. Note that the magnitudeof DNN parameterscanhave signiﬁcant\n",
      "changeduringtrainingwhenthenetworksizeissmall(WehavemorediscussioninDiscussion).\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "url = \"https://arxiv.org/pdf/1808.04295\"\n",
    "paperFilePath = getPaper(url)\n",
    "paperFilePath = \"random_paper.pdf\"\n",
    "paperContent = pdfplumber.open(paperFilePath).pages\n",
    "displayPaperContent(paperContent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf677aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We study DNN training by Fourier analysis. Our theoretical framework explains why DNNs often endow low-frequency components of the target function with a higher priority during the training. Small initialization leads to good generalization ability of DNN while preserving the DNN’s ability to fit any function. These results are further conﬁrmed by experiments of DNNs fitting the following datasets, that is, natural images, one-dimensional functions and MNIST dataset.\n",
      "\n",
      " The loss function is the sum of the squared errors between the target function and the DNN output.\n",
      "TheDNNwithonehiddenlayerusingtanhfunctionasactivationfunctionis\n",
      "\n",
      " We propose a theoretical framework to understand deep learning, and show that it explains the training behavior of DNNs.\n",
      "\n",
      " The DNN training process is governed by the weights rather than the biases.\n",
      " We prove that the Fourier transform of the loss function of a DNN is the Fourier transform of the target function. We also prove that the Fourier transform of the loss function of a DNN is the Fourier transform of the gradient of the loss function. This is a generalization of the F-Principle. We also prove that the Fourier transform of the loss function of a DNN is the Fourier transform of the gradient of the gradient of the loss function. This is a generalization of the F-Principle. We also prove that the Fourier transform of the loss function of a DNN is the Fourier transform of the gradient of the gradient of the gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " The paper is quite readable. The main result is that the gradient of the loss function is a convex function of the parameters. This is a very strong result, and it means that the gradient of the loss function is a good way to optimize the parameters. The paper also shows that the gradient of the loss function is a good way to optimize the parameters in a non-convex setting. This is a weaker result, but it is still very interesting. The paper also shows that the gradient of the loss function is a good way to optimize the parameters in a non-convex setting. This is a weaker result, but it is still very interesting. The paper also shows that the\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "showPaperSummary(paperContent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd617483",
   "metadata": {},
   "source": [
    "## Run tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1acfe98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
